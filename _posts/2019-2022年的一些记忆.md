---
layout: single
title:  "2019-2022年的一些记忆"
---
离开上一家公司(farfetch)也有一年有余。 借此机会，记录一下这几年的经验。

## Farfetch我负责过的业务
  
  榜单服务和年度账单服务是我在发发奇负责的业务。 

### WHAT
* 榜单业务
  文字描述会很突兀，先给出两张图
* ![board_pic_1](/assets/images/farfetch_board01.jpg)

* ![board_pic_2](/assets/images/farfetch_board02.jpg)

通过图可知，该业务是通过提供商品不同维度的特征，帮助用户做下单决策。初略的导购路径可分为如下三种
  * app/wechat 首页 -> 榜单主页
  * 活动促销链接 -> 榜单主页
  * App/WeChat -> 不同功能模块 -> PDP/PLP -> 榜单主页

在此基础上，我们来看看架构草图
    
![board_arch_v2](/assets/images/board_arch_v2.png)

  * 整个系统为不同的端应用分别提供channel服务。
  * 计算引擎使用了spark的miro-stream实现批处理和流处理业务。通过分布式文件系统和kafka与数据中台交互数据
  * 核心业务服务器是典型的tomcat。通过消息队列(实时数据)和数据库(离线数据)交互数据与计算引擎交互数据。利用HTTP和消息队列与其它领域系统进行交互

### 不足
  * 过度依赖分布式缓存
    * 缓存宕机会使展示端无法展示榜单信息
    * 高写并发写会使缓存延迟变高，进而影响其它业务的缓存正常使用
  * spark批处理作业给数据库的写压力过大
    * 批处理作业的写入量可以达到50W+，写入会拖慢数据库响应
  * web容器吞吐量瓶颈
    * 消息推送是导致此问题的罪魁祸首。(100W的用户20%的回访率就回达到20W的并发用户数)

### 初步改进
  * 分布式缓存
    * 增加了内存缓存和缓存数据库备份，提供双保险。
  * 批处理作业
    * 通过消息队列进行解耦。
  * 容器吞吐量瓶颈
    * 业务上优化推送规则，一次性推送变更为阶段性批量推送。

### 后续改进点
  此系统，已在线稳定运行4年多。但是，性能槽点还是特别显眼。
1. 可以对tomcat做一些系统性的性能测试。现阶段，单机容器吞吐量无法突破1000，Tomcat使用了默认200的并发度没有做实际的benchmarking。
2. 离线数据库写入没有进行限流，导致数据库和分布式缓存都会出现突发流量。后续虽然有消息队列缓解，但是，Kafka的处理能力还是远高于数据的能力，这部分需要进一步控制
3. 活动期间读流量瓶颈，可以通过网关层的缓存缓解。无脑增加无状态服务的workaround不是一个solution。
4. 代码结构落后，现在使用的3层结构 API -> Service -> DAO缺乏扩展性，业务的改动需要改动大部分层次的代码。后续可引入领域层，做到业务越发散，领域越收敛。

改进后的架构，如下
   
![board_arch_v3](/assets/images/board_arch_v3.png)

改进后的变化点如下
1. 网关层集成分布式缓存，消除web容器读请求吞吐量瓶颈。网关层的吞吐量是web服务器的100倍左右。
2. 引入CDC工具，解决服务器直接批量读写持久层的问题。


==========================================================================================

今天先写到这。
2023/09/27


* 年度账单业务

## 初步优化



### 后续的思考
  😩









